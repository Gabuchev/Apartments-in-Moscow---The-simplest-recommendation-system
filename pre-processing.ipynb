{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#!pip install tqdm\n",
    "import tqdm \n",
    "#from tqdm.auto\n",
    "#import trange\n",
    "import nltk\n",
    "import re\n",
    "data = pd.read_csv('E:\\project university\\Выпускная работа\\Сlean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\daniel\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\daniel\\anaconda3\\lib\\site-packages (from tqdm) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=data['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Предобработка текста\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "def token_and_stem(text):\n",
    "    tokens=[word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems=[stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "#расширяю список стоп-слов\n",
    "stopwords.extend(['что','это','так', 'вот', 'быть','как','в', 'к','на', 'спешите', 'внимание', 'лучшее', 'самое','Успейте','выгодно', 'Прекрасная'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "n_featur=200000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000,\n",
    "                                 min_df=0.01, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=token_and_stem, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['бол', 'больш', 'будт', 'быт', 'вед', 'вниман', 'впроч', 'всег', 'всегд', 'выгодн', 'даж', 'друг', 'е', 'ег', 'ем', 'есл', 'ест', 'ещ', 'зач', 'зде', 'ил', 'иногд', 'когд', 'конечн', 'куд', 'лучш', 'межд', 'мен', 'мног', 'мо', 'можн', 'нег', 'нельз', 'нибуд', 'никогд', 'нич', 'опя', 'посл', 'пот', 'почт', 'прекрасн', 'разв', 'сво', 'себ', 'совс', 'спеш', 'теб', 'тепер', 'тог', 'тогд', 'тож', 'тольк', 'усп', 'хорош', 'хот', 'чег', 'чут', 'эт'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_matrix=tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100723x2743 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10074824 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_matrix)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100723, 2743)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x2743 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 372 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 909)\t0.22509366481195017\n",
      "  (0, 147)\t0.3491669195051581\n",
      "  (0, 891)\t0.35250970697315975\n",
      "  (0, 1505)\t0.2769665067395238\n",
      "  (0, 2471)\t0.23083568113736427\n",
      "  (0, 1272)\t0.22606472521896948\n",
      "  (0, 163)\t0.29007048296817806\n",
      "  (0, 1260)\t0.11803167122026172\n",
      "  (0, 1572)\t0.17023019331294503\n",
      "  (0, 1185)\t0.2060986247126012\n",
      "  (0, 1105)\t0.17073786753370457\n",
      "  (0, 144)\t0.16695872176703433\n",
      "  (0, 81)\t0.2176076463371947\n",
      "  (0, 615)\t0.177039658219326\n",
      "  (0, 1149)\t0.20460438547022824\n",
      "  (0, 541)\t0.10272993898550446\n",
      "  (0, 882)\t0.17413337838089415\n",
      "  (0, 1504)\t0.26889818718221464\n",
      "  (0, 1891)\t0.24850803821122316\n",
      "  (1, 172)\t0.21323391566572278\n",
      "  (1, 2651)\t0.2860707404709519\n",
      "  (1, 365)\t0.20490021376495055\n",
      "  (1, 171)\t0.2084209534561087\n",
      "  (1, 2036)\t0.16079258168830468\n",
      "  (1, 586)\t0.23110048484425416\n",
      "  :\t:\n",
      "  (100722, 1224)\t0.2175765726836384\n",
      "  (100722, 1223)\t0.2084001598688728\n",
      "  (100722, 1801)\t0.11723946358722705\n",
      "  (100722, 1341)\t0.14450556603418255\n",
      "  (100722, 983)\t0.13080951064128046\n",
      "  (100722, 982)\t0.12638755000763305\n",
      "  (100722, 723)\t0.13914758062125707\n",
      "  (100722, 1476)\t0.16572581625756239\n",
      "  (100722, 906)\t0.17842985341728307\n",
      "  (100722, 1717)\t0.14157918575537107\n",
      "  (100722, 905)\t0.17721549251398433\n",
      "  (100722, 58)\t0.17231610313738607\n",
      "  (100722, 2735)\t0.15392216040328066\n",
      "  (100722, 857)\t0.10811067485004218\n",
      "  (100722, 598)\t0.13442779383931605\n",
      "  (100722, 1474)\t0.10605031269552459\n",
      "  (100722, 1607)\t0.15406696143572518\n",
      "  (100722, 834)\t0.20198016871860372\n",
      "  (100722, 271)\t0.12046840836403205\n",
      "  (100722, 1714)\t0.08665885287682656\n",
      "  (100722, 1473)\t0.1015148647143449\n",
      "  (100722, 2722)\t0.11585677701598594\n",
      "  (100722, 2697)\t0.07836541978652378\n",
      "  (100722, 1807)\t0.15094095941579896\n",
      "  (100722, 882)\t0.051272969721459616\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89dcf862c852db1b0a432862e964b9951f6434871ae0e32fb598fac69ceb7a59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
