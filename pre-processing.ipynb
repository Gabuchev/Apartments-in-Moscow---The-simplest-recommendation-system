{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('E:\\project university\\–í—ã–ø—É—Å–∫–Ω–∞—è —Ä–∞–±–æ—Ç–∞\\–°lean.csv')\n",
    "data=df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "\n",
    "import re\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "patterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~‚Äî\\\"\\-]+\"\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "#—Ä–∞—Å—à–∏—Ä—è—é —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "stopwords_ru.extend(['—á—Ç–æ','—ç—Ç–æ','—Ç–∞–∫', '–≤–æ—Ç', '–±—ã—Ç—å','–∫–∞–∫','–≤', '–∫','–Ω–∞', '—Å–ø–µ—à–∏—Ç–µ', \n",
    "'–≤–Ω–∏–º–∞–Ω–∏–µ', '–ª—É—á—à–µ–µ', '—Å–∞–º–æ–µ','–£—Å–ø–µ–π—Ç–µ','–≤—ã–≥–æ–¥–Ω–æ', '–ü—Ä–µ–∫—Ä–∞—Å–Ω–∞—è','üõë','‚ùáÔ∏è','üëá','‚öúÔ∏è','ü§©','—Å—É–ø–µ—Ä','‚úî','–ª–µ–≥–µ–Ω–¥–∞—Ä–Ω—ã–π','üîë','–ø—Ä–æ–¥–∞–≤–∞—Ç—å—Å—è','‚úÖ','–ø—Ä–æ–¥–∞–≤–∞—Ç—å',\n",
    "'üçÄ','üìå','üî•','‚ùóÔ∏è','–ø–æ–¥–∞—Ä–æ–∫','‚ùï','üìç','üî¥','üè¶','üéÅ','‚û°Ô∏è','‚òéÔ∏è'])\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokens.append(token)\n",
    "    if len(tokens) > 2:\n",
    "        return tokens\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.apply(lemmatize)\n",
    "#df['preprocessed_review'] = df['review'].apply(lambda review: data_preprocessing(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [–æ–¥–Ω–æ–∫–æ–º–Ω–∞—Ç–Ω—ã–π, –∫–≤–∞—Ä—Ç–∏—Ä–∞, –¥–æ–º, –ª–∏—Ñ—Ç, –º—É—Å–æ—Ä–æ–ø—Ä–æ...\n",
       "1    [—Å–≤–µ—Ç–ª—ã–π, –ø—Ä–æ—Å—Ç–æ—Ä–Ω—ã–π, –∫–æ–º–Ω–∞—Ç–∞, –∫–≤, –º–µ—Ç—Ä, –æ—Ç–¥–µ–ª...\n",
       "2    [–ø–µ—Ä–≤–æ–º–∞–π—Å–∫–∏–π, –º–∏–Ω–∞, –ø–µ—à–∫–æ–º—Ç—ë–ø–ª—ã–π, —É—é—Ç–Ω—ã–π, –∫–≤–∞...\n",
       "3    [—à–∞–≥–æ–≤—ã–π, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, –ø–∞—Ä–∫, —Å–æ–∫–æ–ª—å–Ω–∏–∫, —Ä–∞–∑–≤–∏—Ç...\n",
       "4    [–∏–ª—å–∏–Ω—Å–∫–∏–π, –≤—Å—ë, –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π, –∫–≤...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ú–µ–Ω—è—é –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "df = df.fillna(\"[' –æ ']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('PRE.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ 10 —Å–∞–º—ã—Ö –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "for tokens in df.iloc[:]:\n",
    "    for token in tokens:\n",
    "        word_freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42190"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–∫–≤–∞—Ä—Ç–∏—Ä–∞',\n",
       " '–≤',\n",
       " '–¥–æ–º',\n",
       " '–º',\n",
       " '–¥–µ—Ç—Å–∫–∏–π',\n",
       " '–∫–æ–º–ø–ª–µ–∫—Å',\n",
       " '—ç—Ç–∞–∂',\n",
       " '–º–∏–Ω—É—Ç–∞',\n",
       " '–¥–≤–æ—Ä',\n",
       " '–º–µ—Ç—Ä–æ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10 —Å–∞–º—ã—Ö –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤\n",
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13de0658846510013ada92a313cc691cda5658cefa7b17615f36fa1d8fe96911"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
